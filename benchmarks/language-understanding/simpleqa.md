# SimpleQA

> 简单问答评测集，评估基础问答能力

## 📋 基本信息

| 项目 | 内容 |
|------|------|
| **分类** | Intelligence - 语言理解 |
| **测试条目数** | 4,326题 |
| **输入长度** | 1-2个句子（简单问题） |
| **输出长度** | 简短答案或选项 |
| **评测时间** | 约30-60分钟 |
| **覆盖场景** | 基础问答能力评估 |

---

## 📖 评测集介绍

SimpleQA是由OpenAI于2024年10月发布的基准测试集，旨在评估大型语言模型回答简短事实性问题的能力。该数据集包含4,326个问题，每个问题都有一个明确且无可争议的答案，便于自动评分。

### 特点

- **高正确性**: 所有问题的参考答案均由两名独立的AI训练师验证，确保答案的准确性和一致性
- **主题多样性**: 涵盖从科学技术到电视节目和电子游戏等广泛主题，确保模型在不同领域的表现均能得到评估
- **前沿挑战性**: 与早期基准测试（如TriviaQA和NQ）相比，SimpleQA对于前沿模型提出了更高的挑战。例如，GPT-4o在该数据集上的得分不足40%
- **高效用户体验**: 问题和答案简洁明了，便于快速运行和评分，支持通过OpenAI API或其他前沿模型API进行高效评估
- **输入输出**: 输入为1-2个句子的简单问题，输出为简短答案或选项

### 适用场景

- 基础问答能力评估
- 模型快速能力测试
- 简单任务基准测试

### 适用模型

- **所有规模模型**: 适合所有规模的模型，是基础能力评测集
- **小模型（<1B）**: 推荐使用，评估基础问答能力
- **大模型（>10B）**: 可以使用，但可能过于简单

---

## 🔗 资源链接

- **GitHub**: [SimpleQA (simple-evals)](https://github.com/openai/simple-evals)
- **论文**: [SimpleQA论文](https://arxiv.org/abs/2411.04368)
- **OpenAI官方博客**: [Introducing SimpleQA](https://openai.com/index/introducing-simpleqa/)

---

## 📖 使用指南

### 1. 数据准备

```python
# 从GitHub仓库下载
# git clone https://github.com/openai/simple-evals.git

# 或使用simple-evals库
# pip install simple-evals
from simple_evals import SimpleQA

dataset = SimpleQA()
```

### 2. 运行评测

#### 使用OpenCompass

```python
from opencompass import OpenCompass

config = {
    'dataset': 'simpleqa',
    'model': 'your_model',
    'temperature': 0
}

result = OpenCompass.run(config)
```

### 3. 结果分析

- **准确率**: 计算正确答案的比例
- **回答质量**: 分析回答的准确性和完整性
- **错误分析**: 分析错误类型和原因

---

## ⚙️ 评测参数

- **Temperature**: 0（确定性输出）
- **测试次数**: 通常测试1次
- **Max Tokens**: 根据问题复杂度，通常50-200

---

## 💡 最佳实践

1. **完整评测**: 建议评测所有题目
2. **质量分析**: 不仅关注准确率，还关注回答质量
3. **错误分析**: 分析错误案例，找出模型弱点

---

## 📝 输入输出示例

> **注意**: 以下示例基于SimpleQA数据集的典型格式，用于说明评测集的结构。真实题目请从SimpleQA数据集获取。

### 示例1: 事实性问答

**输入（Input）**:
```
问题: 中国的首都是哪里？
```

**期望输出（Expected Output）**:
```
北京
```

### 示例2: 定义类问答

**输入（Input）**:
```
问题: 什么是人工智能？
```

**期望输出（Expected Output）**:
```
人工智能（AI）是计算机科学的一个分支，旨在创建能够执行通常需要人类智能的任务的系统。
```

### 示例3: 选择题

**输入（Input）**:
```
问题: 以下哪个是最大的海洋？

A. 太平洋
B. 大西洋
C. 印度洋
D. 北冰洋
```

**期望输出（Expected Output）**:
```
A. 太平洋
```

---

## 📊 参考结果

| 模型 | 准确率 | 备注 |
|------|--------|------|
| o1-preview | 63.8% | 参考值 |
| GPT-4o | <40% | 参考值，显示数据集的挑战性 |
| GPT-4o mini | 37.6% | 参考值 |

*注：以上为参考值，实际结果可能因评测环境而异。SimpleQA对前沿模型具有较高挑战性。*

---

## 📚 相关资源

- [Intelligence维度指南](../../../intelligence/README.md)
- [语言理解评测集](README.md)

---

**最后更新**: 2025-01-XX

---

## 📝 评估方法

SimpleQA使用经过提示的ChatGPT分类器来评估模型预测的答案。该分类器将预测答案分为三类：
- **正确**: 答案正确且完整
- **错误**: 答案不正确
- **未尝试**: 模型未能提供答案

评估目标是最大化正确率并最小化错误回答，同时关注模型的"未尝试"率，以评估其对自身知识边界的认知能力。

