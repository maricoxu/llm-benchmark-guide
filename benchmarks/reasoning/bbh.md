# BBH (BIG Bench Hard)

> BIG Bench的困难子集，混合推理任务

## 📋 基本信息

| 项目 | 内容 |
|------|------|
| **分类** | Intelligence - 推理能力 |
| **测试条目数** | 23个任务（BIG Bench的子集） |
| **输入长度** | 根据任务类型变化，通常1-5个句子 |
| **输出长度** | 根据任务类型，可能是单词、短语或句子 |
| **评测时间** | 约3-6小时（23个任务） |
| **覆盖场景** | 混合推理任务评估 |

---

## 📖 评测集介绍

BBH（BIG Bench Hard）是BIG Bench的一个子集，由其中的23个更具挑战性的任务组成，用于评估模型在复杂推理任务上的能力。

### 特点

- **困难任务**: BIG Bench中最具挑战性的任务
- **多样化**: 涵盖多种推理类型
- **综合评估**: 评估综合推理能力

### 适用场景

- 复杂推理能力评估
- 混合推理任务评估
- 综合推理能力评估

---

## 🔗 资源链接

- **GitHub**: [BBH](https://github.com/suzgunmirac/BIG-Bench-Hard)
- **BIG Bench**: [BIG Bench](https://github.com/google/BIG-bench)
- **论文**: 待定

---

## 📖 使用指南

### 1. 数据准备

```python
# 待定具体的数据加载方法
```

### 2. 运行评测

#### 使用OpenCompass

```python
from opencompass import OpenCompass

config = {
    'dataset': 'bbh',
    'model': 'your_model',
    'temperature': 0
}

result = OpenCompass.run(config)
```

### 3. 结果分析

- **任务分析**: 分析不同任务的表现
- **推理类型分析**: 分析不同推理类型的表现
- **错误分析**: 分析错误案例

---

## ⚙️ 评测参数

- **Temperature**: 0（确定性输出）
- **测试次数**: 通常测试1次
- **覆盖模型**: DeepSeek-R1-0528-bf16等

---

## 📝 输入输出示例

### 示例1: 逻辑推理

**输入（Input）**:
```
问题: 如果所有的鸟都会飞，企鹅是鸟，那么企鹅会飞吗？

请给出答案和推理过程。
```

**期望输出（Expected Output）**:
```
不会。

推理过程：
虽然"所有的鸟都会飞"和"企鹅是鸟"这两个前提都是真的，
但实际上企鹅不会飞。这说明"所有的鸟都会飞"这个前提是错误的。
因此，企鹅作为鸟，不会飞。
```

### 示例2: 因果推理

**输入（Input）**:
```
问题: 如果下雨，地面会湿。现在地面是湿的，那么一定下雨了吗？

请给出答案和推理过程。
```

**期望输出（Expected Output）**:
```
不一定。

推理过程：
"如果下雨，地面会湿"是一个条件语句，但反过来不一定成立。
地面湿可能是因为其他原因，比如洒水、水管漏水等。
所以不能从"地面湿"推出"一定下雨了"。
```

---

## 💡 最佳实践

1. **多任务评估**: BBH包含23个任务，需要全面评估
2. **推理类型分析**: 分析模型在不同推理类型上的表现
3. **错误分析**: 深入分析错误案例，找出模型弱点

---

## 📊 参考结果

| 模型 | 准确率 | 备注 |
|------|--------|------|
| DeepSeek R1 | 待定 | 参考值 |

*注：以上为参考值，实际结果可能因评测环境而异*

---

## 📚 相关资源

- [BBH GitHub](https://github.com/suzgunmirac/BIG-Bench-Hard)
- [Intelligence维度指南](../../../intelligence/README.md)

---

**最后更新**: 2025-11-30

